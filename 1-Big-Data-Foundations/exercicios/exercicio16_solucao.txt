1. Enviar o diretório local “/input/exercises-data/juros_selic” para o HDFS em “/user/aluno/<nome>/data”
```
$ sudo docker exec -it namenode bash
root@namenode:/# hdfs dfs -put /input/exercises-data/juros_selic/ /user/aluno/rafaella/data
root@namenode:/# hdfs dfs -ls /user/aluno/rafaella/data/juros_selic
```
2. Criar o DataFrame jurosDF para ler o arquivo no HDFS “/user/aluno/<nome>/data/juros_selic/juros_selic.json”
```
$ sudo docker exec -it spark bash
root@spark:/# spark-shell
scala> val jurosDF = spark.read.json("/user/aluno/rafaella/data/juros_selic/juros_selic.json")
```
3. Visualizar o Schema do jurosDF
```
scala> jurosDF.printSchema()
root
 |-- data: string (nullable = true)
 |-- valor: string (nullable = true)
```
4. Mostrar os 5 primeiros registros do jutosDF
```
scala> jurosDF.show(5)
+----------+-----+
|      data|valor|
+----------+-----+
|01/06/1986| 1.27|
|01/07/1986| 1.95|
|01/08/1986| 2.57|
|01/09/1986| 2.94|
|01/10/1986| 1.96|
+----------+-----+
scala> jurosDF.show(5,false)
+----------+-----+
|data      |valor|
+----------+-----+
|01/06/1986|1.27 |
|01/07/1986|1.95 |
|01/08/1986|2.57 |
|01/09/1986|2.94 |
|01/10/1986|1.96 |
+----------+-----+
```
5. Contar a quantidade de registros do jurosDF
```
scala> jurosDF.count()
res5: Long = 393
```
6. Criar o DataFrame jurosDF10 para filtrar apenas os registros com o campo “valor” maior que 10
```
scala> val jurosDF10 = jurosDF.where("valor > 10")
jurosDF10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [data: string, valor: string]
scala> jurosDF10.show(10)
+----------+-----+
|      data|valor|
+----------+-----+
|01/01/1987|11.00|
|01/02/1987|19.61|
|01/03/1987|11.95|
|01/04/1987|15.30|
|01/05/1987|24.63|
|01/06/1987|18.02|
|01/11/1987|12.92|
|01/12/1987|14.38|
|01/01/1988|16.78|
|01/02/1988|18.35|
+----------+-----+
```
7. Salvar o DataFrame jurosDF10  como tabela Hive “<nome>.tab_juros_selic”
```
scala> jurosDF10.write.saveAsTable("rafaella.tab_juros_selic")
```
8. Criar o DataFrame jurosHiveDF para ler a tabela “<nome>.tab_juros_selic”
```
scala> val jurosHiveDF = spark.read.table("rafaella.tab_juros_selic")
jurosHiveDF: org.apache.spark.sql.DataFrame = [data: string, valor: string]
```
9. Visualizar o Schema do jurosHiveDF
```
scala> jurosHiveDF.printSchema()
root
 |-- data: string (nullable = true)
 |-- valor: string (nullable = true)
```
10. Mostrar os 5 primeiros registros do jurosHiveDF
```
scala> jurosHiveDF.show(5)
+----------+-----+
|      data|valor|
+----------+-----+
|01/01/1987|11.00|
|01/02/1987|19.61|
|01/03/1987|11.95|
|01/04/1987|15.30|
|01/05/1987|24.63|
+----------+-----+
```
11. Salvar o DataFrame jurosHiveDF no HDFS no diretório “/user/aluno/nome/data/save_juros” no formato parquet
```
scala> jurosHiveDF.write.save("/user/aluno/rafaella/data/save_juros")
```
12. Visualizar o save_juros no HDFS
```
root@namenode:/# hdfs dfs -ls /user/aluno/rafaella/data/save_juros
```
13. Criar o DataFrame jurosHDFS para ler o diretório do “save_juros” da questão 8
```
scala> val jurosHDFS = spark.read.load("/user/aluno/rafaella/data/save_juros")
jurosHDFS: org.apache.spark.sql.DataFrame = [data: string, valor: string]
```
14. Visualizar o Schema do jurosHDFS
```
scala> jurosHDFS.printSchema()
root
 |-- data: string (nullable = true)
 |-- valor: string (nullable = true)
```
15. Mostrar os 5 primeiros registros do jurosHDFS
```
scala> jurosHDFS.show(5)
+----------+-----+
|      data|valor|
+----------+-----+
|01/01/1987|11.00|
|01/02/1987|19.61|
|01/03/1987|11.95|
|01/04/1987|15.30|
|01/05/1987|24.63|
+----------+-----+
```